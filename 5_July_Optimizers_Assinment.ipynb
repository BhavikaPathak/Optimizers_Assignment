{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 1: Understanding Optimizers**\n",
        "## ANSWER 1\n",
        "Optimization algorithms play a crucial role in training artificial neural networks (ANNs) by helping adjust the model's parameters to minimize a defined loss function. The primary goal of these algorithms is to find the set of parameters that results in the best model performance. They are necessary because training ANNs is a high-dimensional optimization problem with a non-convex loss landscape, making it challenging to find the optimal parameters manually. Optimization algorithms automate the process of finding the optimal model parameters by iteratively updating them based on the gradients of the loss with respect to the parameters.\n",
        "## ANSWER 2\n",
        "Gradient Descent and Its Variants:\n",
        "\n",
        "Gradient Descent: Gradient descent is a fundamental optimization algorithm used in training ANNs. It works by iteratively updating model parameters in the direction of steepest descent of the loss function with respect to those parameters. The basic update rule for gradient descent is: θ = θ - α * ∇L(θ), where θ represents model parameters, α is the learning rate, and ∇L(θ) is the gradient of the loss function.\n",
        "\n",
        "Variants of Gradient Descent: Several variants of gradient descent have been developed to address its limitations:\n",
        "\n",
        "1. Stochastic Gradient Descent (SGD): In SGD, instead of using the entire training dataset in each iteration, a random mini-batch is used, which makes it computationally more efficient.\n",
        "2. Mini-Batch Gradient Descent: This is a compromise between full-batch gradient descent and SGD, where updates are computed using a small random subset of the training data.\n",
        "3. Adam (Adaptive Moment Estimation): Adam combines the ideas of momentum and adaptive learning rates to achieve faster convergence.\n",
        "4. RMSprop (Root Mean Square Propagation): RMSprop adapts the learning rates individually for each parameter, making it more suitable for non-stationary problems.\n",
        "\n",
        "Differences and Trade-offs:\n",
        "\n",
        "The choice of optimization algorithm depends on the specific problem and dataset. Vanilla gradient descent is computationally expensive but can converge to a good solution. SGD and its variants are computationally efficient but may have noisy convergence. Adam and RMSprop adapt learning rates, which can lead to faster convergence but may require more memory.\n",
        "## ANSWER 3\n",
        "Challenges of Traditional Gradient Descent Optimization:\n",
        "\n",
        "a. Slow Convergence: Traditional gradient descent methods often have slow convergence because they rely on small fixed step sizes (learning rates) to update model parameters. In deep neural networks, the optimization process can be time-consuming, requiring a large number of iterations to reach an optimal solution.\n",
        "\n",
        "b. Local Minima: The loss landscape in high-dimensional parameter spaces can contain numerous local minima. Traditional gradient descent methods are susceptible to getting stuck in these local minima, resulting in suboptimal solutions.\n",
        "\n",
        "Modern Optimizers Address These Challenges:\n",
        "\n",
        "Adaptive Learning Rates: Many modern optimization algorithms, such as Adam and RMSprop, adapt the learning rates for each parameter individually. This adaptation allows for larger steps in directions with flat or large gradients and smaller steps in directions with steep or oscillatory gradients. It speeds up convergence by avoiding overly conservative updates and compensating for the slow convergence in some directions.\n",
        "\n",
        "Momentum: Momentum, as a concept within modern optimizers, helps to mitigate slow convergence by introducing a moving average of past gradients. This moving average acts as an additional force that keeps the optimization process moving in a consistent direction and prevents oscillations. Momentum is particularly effective in escaping shallow local minima and accelerating convergence.\n",
        "## ANSWER 4\n",
        "Momentum and Learning Rate in Optimization Algorithms:\n",
        "\n",
        "Momentum: Momentum is a technique used in optimization algorithms to address issues like slow convergence and local minima. It introduces a concept of inertia into parameter updates. The momentum term accumulates a fraction of past gradients to determine the direction and speed of updates. This helps the optimizer maintain a consistent direction and overcome local minima, as well as dampening oscillations in the optimization process.\n",
        "\n",
        "Learning Rate: The learning rate is a hyperparameter that determines the step size in parameter updates. It plays a significant role in the convergence and stability of the optimization process. A high learning rate allows for larger steps, potentially leading to faster convergence, but it may also lead to overshooting and instability. A low learning rate provides stability but may slow down convergence and risk getting stuck in local minima.\n",
        "\n",
        "Impact on Convergence and Model Performance:\n",
        "\n",
        "Momentum accelerates convergence by preventing the optimizer from slowing down or getting stuck in local minima. It allows the optimizer to continue moving in the previous direction, even when the gradient alone might suggest a different path. This helps escape local minima and achieve faster convergence.\n",
        "\n",
        "Learning rate directly influences the speed of convergence and the stability of the optimization process. A well-chosen learning rate can lead to faster training with less risk of divergence, but selecting an inappropriate learning rate can lead to slow convergence or instability. Learning rate scheduling, which adjusts the learning rate during training, can help strike a balance between speed and stability."
      ],
      "metadata": {
        "id": "gT555NPBwjd0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 2 : Optimizers Techniques**\n",
        "## ANSWER 5\n",
        "Stochastic Gradient Descent (SGD):\n",
        "\n",
        "Stochastic Gradient Descent (SGD) is a variant of the traditional gradient descent optimization algorithm. In SGD, instead of computing the gradient using the entire training dataset in each iteration, it randomly selects a small subset, known as a mini-batch, to estimate the gradient. This introduces randomness and noise into the parameter updates. SGD has the following advantages and limitations:\n",
        "\n",
        "Advantages:\n",
        "\n",
        "Faster Convergence: The use of mini-batches allows for faster updates of model parameters, making SGD computationally more efficient than traditional gradient descent.\n",
        "Escape Local Minima: The noise introduced by mini-batch sampling can help SGD escape local minima and find better solutions.\n",
        "Regularization: The inherent noise in SGD acts as a form of implicit regularization, preventing overfitting in some cases.\n",
        "\n",
        "Limitations:\n",
        "\n",
        "Noisy Updates: The stochastic nature of SGD can result in noisy updates, making the optimization path more erratic and harder to control.\n",
        "Convergence Variability: The convergence of SGD can be highly variable and dependent on the choice of the learning rate. Tuning the learning rate can be challenging.\n",
        "May Require More Iterations: Due to the randomness in updates, SGD may require more iterations to converge to a solution than traditional gradient descent.\n",
        "Suitability:\n",
        "\n",
        "SGD is well-suited for large datasets where computing the gradient on the entire dataset in each iteration is computationally expensive.\n",
        "It is often used in deep learning and neural network training, where mini-batch updates help achieve faster convergence and can escape local minima.\n",
        "## ANSWER 6\n",
        "Adam Optimizer:\n",
        "\n",
        "The Adam optimizer combines the concepts of momentum and adaptive learning rates to address some of the challenges associated with gradient-based optimization. It maintains two moving averages, one for the first moment (like momentum) and another for the second moment of the gradients. The key components of Adam are as follows:\n",
        "\n",
        "Momentum: Adam includes a momentum term that accumulates a moving average of past gradients. This helps smooth out the optimization process and prevents getting stuck in local minima.\n",
        "\n",
        "Adaptive Learning Rates: Adam adjusts the learning rates individually for each parameter based on the estimated second moment of the gradients. It provides larger updates for parameters with small gradients and smaller updates for parameters with large gradients.\n",
        "\n",
        "Benefits:\n",
        "\n",
        "Fast Convergence: Adam often converges faster than traditional gradient descent and other optimizers, thanks to its adaptive learning rates and momentum.\n",
        "Robust to Hyperparameters: Adam is less sensitive to the choice of learning rates compared to traditional SGD, making it more user-friendly.\n",
        "Suitable for a wide range of tasks and architectures in deep learning.\n",
        "\n",
        "Potential Drawbacks:\n",
        "\n",
        "Memory Usage: Adam requires more memory to store the moving averages, making it less suitable for memory-constrained environments.\n",
        "Sensitive to Hyperparameters: While Adam is generally robust to learning rate choices, it still has hyperparameters that require tuning, such as the exponential decay rates for the moving averages.\n",
        "## ANSWER 7\n",
        "RMSprop Optimizer:\n",
        "\n",
        "RMSprop (Root Mean Square Propagation) is another optimization algorithm that addresses the challenges of adaptive learning rates. Instead of computing the moving averages of gradients, RMSprop normalizes the gradient for each parameter using a running average of the square of past gradients. The key features of RMSprop are as follows:\n",
        "\n",
        "Adaptive Learning Rates: RMSprop adapts learning rates individually for each parameter, making it well-suited for non-stationary problems where the importance of different parameters may change during training.\n",
        "\n",
        "Smoothing Effect: RMSprop's normalization of gradients has a smoothing effect, which can help improve convergence, especially in deep networks.\n",
        "\n",
        "Simplicity: RMSprop is simpler than Adam, with fewer hyperparameters to tune.\n",
        "\n",
        "Relative Strengths and Weaknesses:\n",
        "\n",
        "Adam is often considered a more advanced and versatile optimizer, suitable for a wide range of problems, while RMSprop is simpler and may be preferred when computational resources or memory are limited.\n",
        "\n",
        "Both optimizers excel in avoiding some of the pitfalls of traditional gradient descent, such as slow convergence and sensitivity to learning rates.\n",
        "\n",
        "The choice between Adam and RMSprop may depend on the specific problem, and it is often beneficial to experiment with both to determine which performs better for a given task."
      ],
      "metadata": {
        "id": "A-d3HAoByv_N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 3 : Appling Optimizer**\n",
        "## ANSWER 8\n"
      ],
      "metadata": {
        "id": "bUduM6dB1DVf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "GPqOM7tDvmMb"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "X = StandardScaler().fit_transform(X)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "qZ2EXvbV1Wwb"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(128, activation='relu', input_shape=(4,)),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(3, activation='softmax')\n",
        "])"
      ],
      "metadata": {
        "id": "DQkJQNeX1bDk"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
        "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "IUzpyEBS1fd2"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train, y_train, epochs=100, validation_data=(X_test, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQzDA4761juF",
        "outputId": "fd902873-0fa7-4826-b112-5efc63f27a27"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "4/4 [==============================] - 2s 167ms/step - loss: 1.0749 - accuracy: 0.3333 - val_loss: 1.0666 - val_accuracy: 0.3333\n",
            "Epoch 2/100\n",
            "4/4 [==============================] - 0s 25ms/step - loss: 1.0382 - accuracy: 0.3333 - val_loss: 1.0274 - val_accuracy: 0.4000\n",
            "Epoch 3/100\n",
            "4/4 [==============================] - 0s 28ms/step - loss: 1.0043 - accuracy: 0.3750 - val_loss: 0.9910 - val_accuracy: 0.6667\n",
            "Epoch 4/100\n",
            "4/4 [==============================] - 0s 41ms/step - loss: 0.9722 - accuracy: 0.5917 - val_loss: 0.9572 - val_accuracy: 0.7667\n",
            "Epoch 5/100\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.9426 - accuracy: 0.6833 - val_loss: 0.9258 - val_accuracy: 0.8333\n",
            "Epoch 6/100\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.9150 - accuracy: 0.7500 - val_loss: 0.8968 - val_accuracy: 0.9000\n",
            "Epoch 7/100\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.8901 - accuracy: 0.8000 - val_loss: 0.8696 - val_accuracy: 0.8667\n",
            "Epoch 8/100\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.8656 - accuracy: 0.8167 - val_loss: 0.8435 - val_accuracy: 0.8667\n",
            "Epoch 9/100\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.8429 - accuracy: 0.8333 - val_loss: 0.8197 - val_accuracy: 0.8667\n",
            "Epoch 10/100\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.8215 - accuracy: 0.8250 - val_loss: 0.7971 - val_accuracy: 0.8667\n",
            "Epoch 11/100\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.8016 - accuracy: 0.8167 - val_loss: 0.7762 - val_accuracy: 0.8667\n",
            "Epoch 12/100\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.7826 - accuracy: 0.8250 - val_loss: 0.7561 - val_accuracy: 0.8667\n",
            "Epoch 13/100\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.7649 - accuracy: 0.8167 - val_loss: 0.7372 - val_accuracy: 0.8667\n",
            "Epoch 14/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.7482 - accuracy: 0.8167 - val_loss: 0.7189 - val_accuracy: 0.8667\n",
            "Epoch 15/100\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.7321 - accuracy: 0.8250 - val_loss: 0.7019 - val_accuracy: 0.8667\n",
            "Epoch 16/100\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.7172 - accuracy: 0.8250 - val_loss: 0.6851 - val_accuracy: 0.8667\n",
            "Epoch 17/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.7029 - accuracy: 0.8167 - val_loss: 0.6695 - val_accuracy: 0.8667\n",
            "Epoch 18/100\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.6890 - accuracy: 0.8250 - val_loss: 0.6543 - val_accuracy: 0.8667\n",
            "Epoch 19/100\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.6760 - accuracy: 0.8250 - val_loss: 0.6395 - val_accuracy: 0.8667\n",
            "Epoch 20/100\n",
            "4/4 [==============================] - 0s 22ms/step - loss: 0.6631 - accuracy: 0.8250 - val_loss: 0.6256 - val_accuracy: 0.8667\n",
            "Epoch 21/100\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.6510 - accuracy: 0.8250 - val_loss: 0.6126 - val_accuracy: 0.8667\n",
            "Epoch 22/100\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.6394 - accuracy: 0.8250 - val_loss: 0.6002 - val_accuracy: 0.8667\n",
            "Epoch 23/100\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.6286 - accuracy: 0.8250 - val_loss: 0.5883 - val_accuracy: 0.8667\n",
            "Epoch 24/100\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.6183 - accuracy: 0.8250 - val_loss: 0.5769 - val_accuracy: 0.8667\n",
            "Epoch 25/100\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.6082 - accuracy: 0.8250 - val_loss: 0.5660 - val_accuracy: 0.8667\n",
            "Epoch 26/100\n",
            "4/4 [==============================] - 0s 22ms/step - loss: 0.5987 - accuracy: 0.8250 - val_loss: 0.5555 - val_accuracy: 0.8667\n",
            "Epoch 27/100\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.5894 - accuracy: 0.8250 - val_loss: 0.5454 - val_accuracy: 0.8667\n",
            "Epoch 28/100\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.5808 - accuracy: 0.8250 - val_loss: 0.5359 - val_accuracy: 0.8667\n",
            "Epoch 29/100\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.5721 - accuracy: 0.8250 - val_loss: 0.5267 - val_accuracy: 0.8667\n",
            "Epoch 30/100\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.5641 - accuracy: 0.8250 - val_loss: 0.5179 - val_accuracy: 0.8667\n",
            "Epoch 31/100\n",
            "4/4 [==============================] - 0s 22ms/step - loss: 0.5564 - accuracy: 0.8250 - val_loss: 0.5094 - val_accuracy: 0.9000\n",
            "Epoch 32/100\n",
            "4/4 [==============================] - 0s 22ms/step - loss: 0.5488 - accuracy: 0.8250 - val_loss: 0.5011 - val_accuracy: 0.9000\n",
            "Epoch 33/100\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 0.5416 - accuracy: 0.8250 - val_loss: 0.4934 - val_accuracy: 0.9000\n",
            "Epoch 34/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.5347 - accuracy: 0.8250 - val_loss: 0.4858 - val_accuracy: 0.9000\n",
            "Epoch 35/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.5279 - accuracy: 0.8333 - val_loss: 0.4784 - val_accuracy: 0.9000\n",
            "Epoch 36/100\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.5212 - accuracy: 0.8333 - val_loss: 0.4715 - val_accuracy: 0.9000\n",
            "Epoch 37/100\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.5151 - accuracy: 0.8333 - val_loss: 0.4648 - val_accuracy: 0.9000\n",
            "Epoch 38/100\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.5091 - accuracy: 0.8333 - val_loss: 0.4581 - val_accuracy: 0.9000\n",
            "Epoch 39/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.5029 - accuracy: 0.8333 - val_loss: 0.4517 - val_accuracy: 0.9000\n",
            "Epoch 40/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.4975 - accuracy: 0.8333 - val_loss: 0.4457 - val_accuracy: 0.9000\n",
            "Epoch 41/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.4918 - accuracy: 0.8333 - val_loss: 0.4397 - val_accuracy: 0.9000\n",
            "Epoch 42/100\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.4862 - accuracy: 0.8333 - val_loss: 0.4340 - val_accuracy: 0.9000\n",
            "Epoch 43/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.4809 - accuracy: 0.8333 - val_loss: 0.4283 - val_accuracy: 0.9000\n",
            "Epoch 44/100\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.4760 - accuracy: 0.8333 - val_loss: 0.4230 - val_accuracy: 0.9000\n",
            "Epoch 45/100\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.4708 - accuracy: 0.8333 - val_loss: 0.4177 - val_accuracy: 0.9000\n",
            "Epoch 46/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.4660 - accuracy: 0.8333 - val_loss: 0.4126 - val_accuracy: 0.9000\n",
            "Epoch 47/100\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.4613 - accuracy: 0.8333 - val_loss: 0.4077 - val_accuracy: 0.9000\n",
            "Epoch 48/100\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.4569 - accuracy: 0.8417 - val_loss: 0.4028 - val_accuracy: 0.9000\n",
            "Epoch 49/100\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.4524 - accuracy: 0.8417 - val_loss: 0.3982 - val_accuracy: 0.9000\n",
            "Epoch 50/100\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.4481 - accuracy: 0.8500 - val_loss: 0.3937 - val_accuracy: 0.9000\n",
            "Epoch 51/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.4438 - accuracy: 0.8417 - val_loss: 0.3893 - val_accuracy: 0.9000\n",
            "Epoch 52/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.4399 - accuracy: 0.8417 - val_loss: 0.3851 - val_accuracy: 0.9000\n",
            "Epoch 53/100\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.4359 - accuracy: 0.8417 - val_loss: 0.3810 - val_accuracy: 0.9000\n",
            "Epoch 54/100\n",
            "4/4 [==============================] - 0s 31ms/step - loss: 0.4320 - accuracy: 0.8417 - val_loss: 0.3770 - val_accuracy: 0.9000\n",
            "Epoch 55/100\n",
            "4/4 [==============================] - 0s 28ms/step - loss: 0.4285 - accuracy: 0.8500 - val_loss: 0.3730 - val_accuracy: 0.9000\n",
            "Epoch 56/100\n",
            "4/4 [==============================] - 0s 27ms/step - loss: 0.4247 - accuracy: 0.8500 - val_loss: 0.3693 - val_accuracy: 0.9000\n",
            "Epoch 57/100\n",
            "4/4 [==============================] - 0s 31ms/step - loss: 0.4212 - accuracy: 0.8500 - val_loss: 0.3655 - val_accuracy: 0.9000\n",
            "Epoch 58/100\n",
            "4/4 [==============================] - 0s 29ms/step - loss: 0.4180 - accuracy: 0.8500 - val_loss: 0.3620 - val_accuracy: 0.9000\n",
            "Epoch 59/100\n",
            "4/4 [==============================] - 0s 24ms/step - loss: 0.4145 - accuracy: 0.8500 - val_loss: 0.3586 - val_accuracy: 0.9000\n",
            "Epoch 60/100\n",
            "4/4 [==============================] - 0s 24ms/step - loss: 0.4115 - accuracy: 0.8417 - val_loss: 0.3552 - val_accuracy: 0.9000\n",
            "Epoch 61/100\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.4079 - accuracy: 0.8417 - val_loss: 0.3517 - val_accuracy: 0.9000\n",
            "Epoch 62/100\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.4048 - accuracy: 0.8417 - val_loss: 0.3484 - val_accuracy: 0.9000\n",
            "Epoch 63/100\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.4015 - accuracy: 0.8417 - val_loss: 0.3452 - val_accuracy: 0.9000\n",
            "Epoch 64/100\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.3985 - accuracy: 0.8417 - val_loss: 0.3420 - val_accuracy: 0.9000\n",
            "Epoch 65/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.3958 - accuracy: 0.8500 - val_loss: 0.3390 - val_accuracy: 0.9000\n",
            "Epoch 66/100\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.3927 - accuracy: 0.8583 - val_loss: 0.3360 - val_accuracy: 0.9000\n",
            "Epoch 67/100\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.3898 - accuracy: 0.8500 - val_loss: 0.3330 - val_accuracy: 0.9000\n",
            "Epoch 68/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.3869 - accuracy: 0.8583 - val_loss: 0.3300 - val_accuracy: 0.9000\n",
            "Epoch 69/100\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.3841 - accuracy: 0.8583 - val_loss: 0.3270 - val_accuracy: 0.9000\n",
            "Epoch 70/100\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.3813 - accuracy: 0.8583 - val_loss: 0.3242 - val_accuracy: 0.9000\n",
            "Epoch 71/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.3787 - accuracy: 0.8583 - val_loss: 0.3213 - val_accuracy: 0.9000\n",
            "Epoch 72/100\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.3763 - accuracy: 0.8583 - val_loss: 0.3186 - val_accuracy: 0.9000\n",
            "Epoch 73/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.3737 - accuracy: 0.8667 - val_loss: 0.3159 - val_accuracy: 0.9000\n",
            "Epoch 74/100\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.3711 - accuracy: 0.8667 - val_loss: 0.3133 - val_accuracy: 0.9000\n",
            "Epoch 75/100\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.3686 - accuracy: 0.8667 - val_loss: 0.3107 - val_accuracy: 0.9000\n",
            "Epoch 76/100\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.3663 - accuracy: 0.8667 - val_loss: 0.3081 - val_accuracy: 0.9333\n",
            "Epoch 77/100\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.3639 - accuracy: 0.8667 - val_loss: 0.3056 - val_accuracy: 0.9333\n",
            "Epoch 78/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.3616 - accuracy: 0.8667 - val_loss: 0.3032 - val_accuracy: 0.9333\n",
            "Epoch 79/100\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.3591 - accuracy: 0.8667 - val_loss: 0.3007 - val_accuracy: 0.9333\n",
            "Epoch 80/100\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.3570 - accuracy: 0.8667 - val_loss: 0.2983 - val_accuracy: 0.9333\n",
            "Epoch 81/100\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.3546 - accuracy: 0.8667 - val_loss: 0.2960 - val_accuracy: 0.9333\n",
            "Epoch 82/100\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.3525 - accuracy: 0.8667 - val_loss: 0.2936 - val_accuracy: 0.9333\n",
            "Epoch 83/100\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.3503 - accuracy: 0.8750 - val_loss: 0.2913 - val_accuracy: 0.9333\n",
            "Epoch 84/100\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.3485 - accuracy: 0.8750 - val_loss: 0.2890 - val_accuracy: 0.9333\n",
            "Epoch 85/100\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.3460 - accuracy: 0.8750 - val_loss: 0.2868 - val_accuracy: 0.9333\n",
            "Epoch 86/100\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.3438 - accuracy: 0.8750 - val_loss: 0.2845 - val_accuracy: 0.9333\n",
            "Epoch 87/100\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.3416 - accuracy: 0.8750 - val_loss: 0.2823 - val_accuracy: 0.9333\n",
            "Epoch 88/100\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.3399 - accuracy: 0.8750 - val_loss: 0.2801 - val_accuracy: 0.9333\n",
            "Epoch 89/100\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.3379 - accuracy: 0.8750 - val_loss: 0.2778 - val_accuracy: 0.9333\n",
            "Epoch 90/100\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.3359 - accuracy: 0.8833 - val_loss: 0.2757 - val_accuracy: 0.9333\n",
            "Epoch 91/100\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.3336 - accuracy: 0.8833 - val_loss: 0.2737 - val_accuracy: 0.9333\n",
            "Epoch 92/100\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.3320 - accuracy: 0.8833 - val_loss: 0.2717 - val_accuracy: 0.9333\n",
            "Epoch 93/100\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.3298 - accuracy: 0.8833 - val_loss: 0.2697 - val_accuracy: 0.9333\n",
            "Epoch 94/100\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.3282 - accuracy: 0.8833 - val_loss: 0.2676 - val_accuracy: 0.9333\n",
            "Epoch 95/100\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.3259 - accuracy: 0.8833 - val_loss: 0.2657 - val_accuracy: 0.9333\n",
            "Epoch 96/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.3240 - accuracy: 0.8833 - val_loss: 0.2637 - val_accuracy: 0.9333\n",
            "Epoch 97/100\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.3221 - accuracy: 0.8833 - val_loss: 0.2618 - val_accuracy: 0.9333\n",
            "Epoch 98/100\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.3202 - accuracy: 0.8833 - val_loss: 0.2600 - val_accuracy: 0.9333\n",
            "Epoch 99/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.3185 - accuracy: 0.8833 - val_loss: 0.2581 - val_accuracy: 0.9333\n",
            "Epoch 100/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.3166 - accuracy: 0.8833 - val_loss: 0.2563 - val_accuracy: 0.9333\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7c031473fe80>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
        "print(f'Test Loss: {test_loss}, Test Accuracy: {test_accuracy}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UQ9C4hJu1n9e",
        "outputId": "9021711a-1b57-418d-9b68-300476c5993b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 47ms/step - loss: 0.2563 - accuracy: 0.9333\n",
            "Test Loss: 0.2563222050666809, Test Accuracy: 0.9333333373069763\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ANSWER 9\n",
        "Choosing the appropriate optimizer for a neural network is a critical decision that can significantly impact the training process and the performance of the model on a given task. Here are some considerations and tradeoffs when selecting an optimizer based on factors such as convergence speed, stability, and generalization performance:\n",
        "\n",
        "Convergence Speed:\n",
        "\n",
        "Adam and RMSprop: These optimizers often converge faster because they adapt the learning rates and incorporate momentum. They can navigate the loss landscape more efficiently and escape local minima.\n",
        "\n",
        "SGD: Traditional SGD can converge more slowly due to the fixed learning rate, but it can be faster if the learning rate is chosen optimally for the specific problem. Learning rate scheduling (e.g., learning rate annealing) can be used to improve convergence speed.\n",
        "\n",
        "Consideration: If training time is a critical factor and you want the model to converge quickly, Adam or RMSprop may be preferable. However, remember that faster convergence does not always equate to a better final model.\n",
        "\n",
        "Stability:\n",
        "\n",
        "Adam and RMSprop: These optimizers are generally more stable during training and less sensitive to the choice of learning rates. They offer automatic adjustments that can prevent divergent behavior.\n",
        "\n",
        "SGD: Traditional SGD can be less stable, especially if the learning rate is not chosen carefully. It can oscillate or diverge if the learning rate is too high.\n",
        "\n",
        "Consideration: If you want a more stable training process that is less reliant on hyperparameter tuning, Adam and RMSprop are attractive options.\n",
        "\n",
        "Generalization Performance:\n",
        "\n",
        "SGD: Traditional SGD, with appropriate learning rate and regularization techniques, may lead to better generalization. It can be more robust against overfitting due to its conservative updates.\n",
        "\n",
        "Adam and RMSprop: These optimizers may converge quickly but are more prone to overfitting if not properly regularized or if the learning rate is too high. Careful monitoring of validation performance and early stopping are recommended.\n",
        "\n",
        "Consideration: If you have a limited amount of data and want to prioritize generalization performance, traditional SGD with learning rate annealing and regularization might be a better choice.\n",
        "\n",
        "Computational Resources:\n",
        "\n",
        "Adam and RMSprop: These optimizers often require more memory and computational resources due to the additional computations for adaptive learning rates and momentum. They may not be suitable for resource-constrained environments.\n",
        "\n",
        "SGD: Traditional SGD is computationally more efficient and requires less memory, making it a better choice for constrained environments.\n",
        "\n",
        "Consideration: If you are working with limited computational resources, traditional SGD might be the only practical choice.\n",
        "\n",
        "Hyperparameter Tuning:\n",
        "\n",
        "Adam and RMSprop: These optimizers are less sensitive to hyperparameter settings (e.g., learning rates) and can perform reasonably well with default values. This simplifies the tuning process.\n",
        "\n",
        "SGD: Traditional SGD can be sensitive to the choice of learning rate and may require extensive hyperparameter tuning.\n",
        "\n",
        "Consideration: If you want an optimizer that is easier to set up without extensive tuning, Adam or RMSprop can be more user-friendly.\n",
        "\n",
        "Problem Characteristics:\n",
        "\n",
        "The nature of the problem, such as stationary or non-stationary data, noisy data, or the presence of outliers, can influence the choice of optimizer. Experimentation with different optimizers is often necessary to determine the best fit for the specific task.\n",
        "Transfer Learning:\n",
        "\n",
        "In transfer learning scenarios, the choice of optimizer may depend on the architecture of the pre-trained model. It's common to fine-tune pre-trained models with different optimizers based on the task and the amount of available data."
      ],
      "metadata": {
        "id": "y8I7e7qv2g0o"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QOXwyCA710Xd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}